{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 4096), found shape=(None, 1, 4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(out_word)\n\u001b[0;32m     95\u001b[0m X1, X2, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(X1), np\u001b[38;5;241m.\u001b[39mvstack(X2), np\u001b[38;5;241m.\u001b[39mvstack(y)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Caption generation function\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_caption\u001b[39m(model, tokenizer, photo_features, max_length):\n",
      "File \u001b[1;32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 4096), found shape=(None, 1, 4096)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000  # This should match the actual vocab size in your tokenizer\n",
    "max_length = 34  # Longest caption length for padding (should be derived from dataset captions)\n",
    "# Example captions dictionary (replace this with actual data loading from your dataset)\n",
    "captions_dict = {\n",
    "    \"img1.jpeg\": [\"A dog running in the field.\", \"A dog is playing outside.\"],\n",
    "    \"img2.jpeg\": [\"A girl on a swing.\", \"A child enjoying a swing in the park.\"],\n",
    "    # Add more image-caption pairs here...\n",
    "}\n",
    "\n",
    "# Create train_data as a list of (image_path, caption) pairs\n",
    "train_data = []\n",
    "for img, captions in captions_dict.items():\n",
    "    for caption in captions:\n",
    "        train_data.append((f\"{img}\", caption))\n",
    "\n",
    "# Initialize captions list from captions_dict\n",
    "captions_list = [caption for captions in captions_dict.values() for caption in captions]\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(captions_list)  # captions_list should contain all captions\n",
    "\n",
    "# Function to extract image features\n",
    "def extract_image_features(model, image_path):\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "    features = model.predict(image, verbose=0)\n",
    "    return features\n",
    "\n",
    "# Load pre-trained VGG16 model and remove the final layer\n",
    "cnn_model = VGG16(weights=\"imagenet\")\n",
    "cnn_model = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
    "\n",
    "# Function to create input-output sequence pairs\n",
    "def create_sequences(tokenizer, max_length, desc, photo_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    desc_seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    for i in range(1, len(desc_seq)):\n",
    "        in_seq, out_seq = desc_seq[:i], desc_seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        X1.append(photo_features)\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# Define the combined CNN-LSTM model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # Image feature branch\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
    "\n",
    "    # Sequence feature branch\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Decoder (merge features from CNN and LSTM)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# Training data preparation\n",
    "X1, X2, y = [], [], []\n",
    "for img_path, caption in train_data:  # train_data should be a list of (image_path, caption) pairs\n",
    "    photo_features = extract_image_features(cnn_model, img_path)\n",
    "    in_img, in_seq, out_word = create_sequences(tokenizer, max_length, caption, photo_features, vocab_size)\n",
    "    X1.append(in_img)\n",
    "    X2.append(in_seq)\n",
    "    y.append(out_word)\n",
    "\n",
    "X1, X2, y = np.vstack(X1), np.vstack(X2), np.vstack(y)\n",
    "model.fit([X1, X2], y, epochs=20, verbose=2)\n",
    "\n",
    "# Caption generation function\n",
    "def generate_caption(model, tokenizer, photo_features, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "    return in_text\n",
    "test_images=['test1.jpeg','test2.jpeg']\n",
    "test_captions=[\"A dog running in the field.\",\"A child enjoying a swing in the park.\"]\n",
    "# Evaluate model on BLEU score\n",
    "for img_path in test_images:  # test_images should be a list of paths to test images\n",
    "    reference = [caption.split() for caption in test_captions[img_path]]  # List of true captions\n",
    "    photo_features = extract_image_features(cnn_model, img_path)\n",
    "    generated = generate_caption(model, tokenizer, photo_features, max_length).split()\n",
    "    bleu_score = sentence_bleu(reference, generated)\n",
    "    print(f\"Image: {img_path}, BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x0000021C6DF126B0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Extract image features\u001b[39;00m\n\u001b[0;32m     34\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmuni karthik\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSEM7\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDLCA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMODEL ANS\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to your image folder\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Example captions dataset (load from file or define manually)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m captions_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg1.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA dog running in the field.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA dog is playing outside.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg2.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA girl on a swing.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA child enjoying a swing in the park.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Add more image-caption pairs here...\u001b[39;00m\n\u001b[0;32m     42\u001b[0m }\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mextract_image_features\u001b[1;34m(image_folder, model)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_folder):\n\u001b[0;32m     21\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, img_name)\n\u001b[1;32m---> 22\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     image \u001b[38;5;241m=\u001b[39m img_to_array(image)\n\u001b[0;32m     24\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\image_utils.py:236\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 236\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mpil_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be path-like or io.BytesIO, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\PIL\\Image.py:3536\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3534\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m   3535\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m-> 3536\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x0000021C6DF126B0>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000  # Adjust based on your dataset\n",
    "max_length = 35     # Set to a reasonable max length for captions\n",
    "\n",
    "# Initialize VGG16 model for feature extraction\n",
    "def extract_image_features(image_folder, model):\n",
    "    features = {}\n",
    "    for img_name in os.listdir(image_folder):\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        features[img_name] = feature.flatten()\n",
    "    return features\n",
    "\n",
    "# Load the pre-trained VGG16 model without top layers\n",
    "cnn_model = VGG16(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "\n",
    "# Extract image features\n",
    "image_folder = r\"C:\\Users\\muni karthik\\Desktop\\SEM7\\DLCA\\MODEL ANS\"  # Replace with the path to your image folder\n",
    "image_features = extract_image_features(image_folder, cnn_model)\n",
    "\n",
    "# Example captions dataset (load from file or define manually)\n",
    "captions_dict = {\n",
    "    \"img1.jpeg\": [\"A dog running in the field.\", \"A dog is playing outside.\"],\n",
    "    \"img2.jpeg\": [\"A girl on a swing.\", \"A child enjoying a swing in the park.\"],\n",
    "    # Add more image-caption pairs here...\n",
    "}\n",
    "\n",
    "# Prepare captions data\n",
    "captions_list = list(captions_dict.values())\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(captions_list)\n",
    "\n",
    "# Prepare training sequences\n",
    "def create_sequences(tokenizer, max_length, desc, photo_features, vocab_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    for i in range(1, len(seq)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        X1.append(photo_features)\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# Prepare data for training\n",
    "X1, X2, y = [], [], []\n",
    "for img_name, caption in captions_dict.items():\n",
    "    if img_name in image_features:\n",
    "        in_img, in_seq, out_word = create_sequences(\n",
    "            tokenizer, max_length, caption, image_features[img_name], vocab_size\n",
    "        )\n",
    "        X1.append(in_img)\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_word)\n",
    "\n",
    "X1, X2, y = np.vstack(X1), np.vstack(X2), np.vstack(y)\n",
    "\n",
    "# Model definition\n",
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# Train model\n",
    "model.fit([X1, X2], y, epochs=20, verbose=2)\n",
    "\n",
    "# Caption generation\n",
    "def generate_caption(model, tokenizer, photo_features, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# Test with a new image\n",
    "test_image = \"path_to_test_image.jpg\"  # Replace with the path to a test image\n",
    "test_features = extract_image_features(\"path_to_images\", cnn_model)[test_image]\n",
    "print(\"Generated Caption:\", generate_caption(model, tokenizer, test_features.reshape(1, -1), max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
